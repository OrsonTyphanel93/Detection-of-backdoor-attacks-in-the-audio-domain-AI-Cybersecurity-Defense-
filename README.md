# Detection-of-backdoor-attacks-in-the-audio-domain-AI-Cybersecurity-Defense-
Detection of backdoor attacks in the audio domain

Deep learning techniques allow speech recognition and speaker identification from the user's voice alone. This is useful for controlling various applications (such as entertainment, cars and homes). However, audio recognition deep learning models can be attacked in ways they should not (for example, by opening websites or turning off lights). An attack on audio DNNs involves adding bad data to a training set, so that the DNN cannot learn as well as it should. This can allow someone to control the predictions of the model without anyone knowing.

With LLMs (large languages Models) and PPO (Dark knowledge , embodiment ; Proximal Policy Optimization, renforcement learning), attackers will further strengthen their cybersecurity attacks (such as backdoor, DDos, sphiging, trigger, spyware, etc.), will our standard detection methods be able to cope with even more polymorphic attacks? The aim of this article is to raise awareness and encourage research in this area and collaboration. 

How will we guard against the proliferation of LLMs (which will undoubtedly be vectors for the spread of modern cybersecurity attacks) in our daily activities? 

- Microsoft: ChatGPT
- Google : LamBDA
- YouSerachEngine: YouChat
- Baidu, Inc: ErnieBot
- Perplexity_ai: Perplexity Chat
- AnthropicAI: Claude
- heyjasperai: Jasper Chat




The backdoored model can still function properly with clean data, making it difficult to identify the presence of the backdoor. Backdoor attacks are particularly dangerous if training is outsourced to a third party, as the third party has access to all the resources used for training, including the data, the model and the training operations. These third parties may have malicious employees who can install backdoors into the model without the user's knowledge.



Poisoning-based backdoor attacks are a type of malicious cyberattack that uses malicious input to train a machine learning model. These attacks can be categorised based on different criteria, such as the image generator and the label shifting function that the attacker uses. To understand the attack, you can look at the 3 risks involved: the standard risk, the backdoor risk and the perceptible risk. Standard risk is whether the infected model can correctly predict benign samples. The backdoor risk measures whether an attacker can achieve his malicious goal by predicting certain samples. The perceptible risk examines whether the poisoned sample is detectable by humans or machines. In summary, there is a unified framework of poisoning-based backdoor attacks in which the attacker takes into account different risks, such as correctly predicting benign samples, achieving malicious goals and detecting poisoned samples.



![fig_plot_audio_comparison](https://user-images.githubusercontent.com/64611605/218340528-41955e0f-d73e-41fb-8585-ace1fe0fb203.png)
![fig_1](https://user-images.githubusercontent.com/64611605/218340613-c96324ca-45d4-43d6-b16e-45c1a9dc795a.png)

![fig_2](https://user-images.githubusercontent.com/64611605/218340618-05bccff7-b29d-4457-b59a-87c2e1d73749.png)

![adv_6](https://user-images.githubusercontent.com/64611605/218340533-e86d5549-e986-45ec-900b-5fd7be41caab.png)
![adv_3](https://user-images.githubusercontent.com/64611605/218340539-45d576bf-748f-4edf-9c6f-e4fcb0b86d83.png)
